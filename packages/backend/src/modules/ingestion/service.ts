import { db } from '../../database/index.js';
import type { LogInput } from '@logward/shared';
import { createQueue, publisher } from '../../queue/connection.js';
import type { LogEntry } from '../sigma/detection-engine.js';

export class IngestionService {
  /**
   * Ingest logs in batch
   */
  async ingestLogs(logs: LogInput[], projectId: string): Promise<number> {
    if (logs.length === 0) {
      return 0;
    }

    // Convert logs to database format
    const dbLogs = logs.map((log) => ({
      time: typeof log.time === 'string' ? new Date(log.time) : log.time,
      project_id: projectId,
      service: log.service,
      level: log.level,
      message: log.message,
      metadata: log.metadata || null,
      trace_id: log.trace_id || null,
    }));

    // Insert logs in batch
    await db
      .insertInto('logs')
      .values(dbLogs)
      .execute();

    // Trigger Sigma detection (async, non-blocking)
    this.triggerSigmaDetection(logs, projectId).catch((err) => {
      console.error('[Ingestion] Failed to trigger Sigma detection:', err);
    });

    // Publish to Redis for live tail
    try {
      // Map back to API format for frontend
      const apiLogs = dbLogs.map(log => ({
        id: undefined, // ID is generated by DB, might be missing here if we don't return it
        time: log.time,
        projectId: log.project_id,
        service: log.service,
        level: log.level,
        message: log.message,
        metadata: log.metadata,
        traceId: log.trace_id,
      }));

      await publisher.publish('logs:new', JSON.stringify({
        projectId,
        logs: apiLogs
      }));
    } catch (error) {
      console.error('[Ingestion] Failed to publish logs to Redis:', error);
    }

    return logs.length;
  }

  /**
   * Trigger Sigma detection job for ingested logs
   */
  private async triggerSigmaDetection(logs: LogInput[], projectId: string): Promise<void> {
    try {
      // Get project to find organization_id
      const project = await db
        .selectFrom('projects')
        .select(['organization_id'])
        .where('id', '=', projectId)
        .executeTakeFirst();

      if (!project) {
        console.warn(`[Ingestion] Project not found: ${projectId}`);
        return;
      }

      // Convert logs to LogEntry format for detection engine
      const logEntries: LogEntry[] = logs.map((log) => ({
        service: log.service,
        level: log.level,
        message: log.message,
        metadata: log.metadata,
        trace_id: log.trace_id,
        time: log.time,
      }));

      // Queue Sigma detection job
      const detectionQueue = createQueue('sigma-detection');

      await detectionQueue.add('detect-logs', {
        logs: logEntries,
        organizationId: project.organization_id,
        projectId,
      });

      console.log(`[Ingestion] Queued Sigma detection for ${logs.length} logs`);
    } catch (error) {
      console.error('[Ingestion] Error triggering Sigma detection:', error);
      // Don't throw - ingestion should succeed even if detection queueing fails
    }
  }

  /**
   * Get log statistics
   */
  async getStats(projectId: string, from?: Date, to?: Date) {
    const query = db
      .selectFrom('logs')
      .select([
        db.fn.count('time').as('total'),
        'level',
      ])
      .where('project_id', '=', projectId)
      .groupBy('level');

    if (from) {
      query.where('time', '>=', from);
    }

    if (to) {
      query.where('time', '<=', to);
    }

    const results = await query.execute();

    return {
      total: results.reduce((sum, r) => sum + Number(r.total), 0),
      by_level: results.reduce((acc, r) => {
        acc[r.level] = Number(r.total);
        return acc;
      }, {} as Record<string, number>),
    };
  }
}

export const ingestionService = new IngestionService();
